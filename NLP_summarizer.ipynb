{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3i5uVDS09HV0rDYEFCs1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devshah21/NLP_summarizer/blob/main/NLP_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W7CdTxAvvZzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e2a73b-d34d-4cb6-8b95-a2821a9b0ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-02 18:09:53.867230: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy import displacy\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "jMOLZwCxwNcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151f81d4-76bf-421f-e04b-889b7fc6c83f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrds = STOP_WORDS\n",
        "print(wrds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evM-vOM2zpvA",
        "outputId": "4d8d6204-804e-48fc-a12b-224d9c8f37f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hers', 'been', 'we', 'fifty', 'however', 'wherein', 'of', 'some', 'nevertheless', 'before', 'already', 'n’t', 'ten', 'off', 'once', 'its', 'any', 'latter', 'thereby', 'full', 'everything', 'in', 'under', '‘d', '‘re', 'keep', 'has', 'two', 'others', 'hereupon', 'whereupon', 'top', 'among', 'she', 'else', 'hereafter', 'indeed', 'who', 'whoever', 'yet', 'seem', 'when', 'least', 'last', 'must', 'along', 'meanwhile', 'well', 'elsewhere', '’m', 'your', 'all', 'several', 'hundred', 'anyone', 'him', 'because', '‘m', 'at', 'somewhere', 'seemed', 'almost', 'everywhere', 'can', 'done', 'upon', 'above', 'call', 'as', 'first', 'across', 'it', 'through', 'but', 'becoming', 'around', 'no', 'ours', 'with', 'mostly', 'hence', 'every', 'was', 'one', 'seeming', 'until', 'against', 'back', 'you', 'between', 'me', 'six', '’d', 'below', 'whereas', 'moreover', 'by', 'without', 'someone', 'never', 'yourselves', 'thus', 'would', 'too', 'towards', 'not', \"'ve\", 'thereupon', 'whose', 'her', 'next', 'out', 'myself', 'nowhere', 'three', 'using', 'enough', 'put', 'during', 'herself', 'sixty', 'these', 'their', 'former', 'made', 'wherever', 'perhaps', 'whence', 'please', \"'ll\", 'if', 'ever', 'otherwise', 'everyone', 'somehow', 'may', 'show', 'go', 'will', 'our', 'since', 'thru', 'very', \"'m\", 'which', 'so', 'become', 'only', 'here', 'an', 'doing', 'that', 'for', 'though', 'amount', 'whole', 'really', 'there', 'just', 'a', 'even', 'again', 'anywhere', 'were', 'together', 'could', 'either', 'take', 'behind', 'five', '’ll', 'toward', \"'re\", 'had', 'whither', 'anyway', 'into', 'onto', '’re', '‘ve', 'my', 'something', 'might', 'them', 'see', 'throughout', 'whether', 'afterwards', 'on', 'while', 'make', 'fifteen', 'down', 'n‘t', 'ca', '‘ll', 'anyhow', 'twelve', 'eight', 'per', 'from', 'those', 'then', 'nor', 'his', 'side', 'nine', 'move', 'although', 'seems', 'whom', \"'d\", 'yours', 'how', 'became', 'themselves', 'mine', 'and', 'after', 'serious', 'say', 'us', 'formerly', 'about', 'none', 'why', 'rather', 'twenty', 'same', 'is', 'get', 'should', 'yourself', 'due', 'thereafter', 'other', 'further', 'bottom', 'whereby', 'whereafter', 'over', 'herein', 'always', 'also', 'used', 'what', 'both', 'regarding', 'alone', 'being', 'itself', 'amongst', 'own', 'do', 'besides', 'eleven', 'part', 'beside', 'various', 'to', 'now', 'this', 'sometimes', 'cannot', 'nobody', '’s', 'most', 'via', 'or', 'beforehand', 'they', 'third', 'have', 'another', '‘s', 'give', 'name', 'within', 'be', 'hereby', 'anything', 'often', 'latterly', 'front', 'he', 'quite', 'where', 'are', '’ve', 'himself', 'thence', 'many', 'such', 'unless', 'empty', \"'s\", 're', 'up', 'than', 'whatever', 'forty', 'i', 'four', 'becomes', 'still', 'namely', 'therein', 'less', 'ourselves', 'except', 'neither', \"n't\", 'the', 'therefore', 'few', 'sometime', 'noone', 'am', 'much', 'beyond', 'does', 'more', 'each', 'did', 'nothing', 'whenever'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampletext = 'This diagram above is known as a parse tree and it is a visual representation of the relationships between all words. In this particular example, we can see that London has a relation with be and capital. From this knowledge alone, we can interpret that London is a capital. This intuitive way of interpreting a parse tree can be used for any of the other words in this sentence. However, a computer wouldn’t just know how to parse each word nor will they know how to generate a parse tree. In order to do this to maximum accuracy, the computer needs to be trained to identify each word and generate these relationships. Similar to how the part-of-speech portion of the NLP pipeline employed Machine Learning algorithms, this part also does. Dependency parsing works by inputting words and sentences through a Machine Learning algorithm/model to generate an output result.'"
      ],
      "metadata": {
        "id": "aXuGFJ5DzyVc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(sampletext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig89pvWb1JHo",
        "outputId": "6eaba169-6ca0-4616-ff13-c6670b716143"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'diagram',\n",
              " 'above',\n",
              " 'is',\n",
              " 'known',\n",
              " 'as',\n",
              " 'a',\n",
              " 'parse',\n",
              " 'tree',\n",
              " 'and',\n",
              " 'it',\n",
              " 'is',\n",
              " 'a',\n",
              " 'visual',\n",
              " 'representation',\n",
              " 'of',\n",
              " 'the',\n",
              " 'relationships',\n",
              " 'between',\n",
              " 'all',\n",
              " 'words',\n",
              " '.',\n",
              " 'In',\n",
              " 'this',\n",
              " 'particular',\n",
              " 'example',\n",
              " ',',\n",
              " 'we',\n",
              " 'can',\n",
              " 'see',\n",
              " 'that',\n",
              " 'London',\n",
              " 'has',\n",
              " 'a',\n",
              " 'relation',\n",
              " 'with',\n",
              " 'be',\n",
              " 'and',\n",
              " 'capital',\n",
              " '.',\n",
              " 'From',\n",
              " 'this',\n",
              " 'knowledge',\n",
              " 'alone',\n",
              " ',',\n",
              " 'we',\n",
              " 'can',\n",
              " 'interpret',\n",
              " 'that',\n",
              " 'London',\n",
              " 'is',\n",
              " 'a',\n",
              " 'capital',\n",
              " '.',\n",
              " 'This',\n",
              " 'intuitive',\n",
              " 'way',\n",
              " 'of',\n",
              " 'interpreting',\n",
              " 'a',\n",
              " 'parse',\n",
              " 'tree',\n",
              " 'can',\n",
              " 'be',\n",
              " 'used',\n",
              " 'for',\n",
              " 'any',\n",
              " 'of',\n",
              " 'the',\n",
              " 'other',\n",
              " 'words',\n",
              " 'in',\n",
              " 'this',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'a',\n",
              " 'computer',\n",
              " 'wouldn',\n",
              " '’',\n",
              " 't',\n",
              " 'just',\n",
              " 'know',\n",
              " 'how',\n",
              " 'to',\n",
              " 'parse',\n",
              " 'each',\n",
              " 'word',\n",
              " 'nor',\n",
              " 'will',\n",
              " 'they',\n",
              " 'know',\n",
              " 'how',\n",
              " 'to',\n",
              " 'generate',\n",
              " 'a',\n",
              " 'parse',\n",
              " 'tree',\n",
              " '.',\n",
              " 'In',\n",
              " 'order',\n",
              " 'to',\n",
              " 'do',\n",
              " 'this',\n",
              " 'to',\n",
              " 'maximum',\n",
              " 'accuracy',\n",
              " ',',\n",
              " 'the',\n",
              " 'computer',\n",
              " 'needs',\n",
              " 'to',\n",
              " 'be',\n",
              " 'trained',\n",
              " 'to',\n",
              " 'identify',\n",
              " 'each',\n",
              " 'word',\n",
              " 'and',\n",
              " 'generate',\n",
              " 'these',\n",
              " 'relationships',\n",
              " '.',\n",
              " 'Similar',\n",
              " 'to',\n",
              " 'how',\n",
              " 'the',\n",
              " 'part-of-speech',\n",
              " 'portion',\n",
              " 'of',\n",
              " 'the',\n",
              " 'NLP',\n",
              " 'pipeline',\n",
              " 'employed',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " 'algorithms',\n",
              " ',',\n",
              " 'this',\n",
              " 'part',\n",
              " 'also',\n",
              " 'does',\n",
              " '.',\n",
              " 'Dependency',\n",
              " 'parsing',\n",
              " 'works',\n",
              " 'by',\n",
              " 'inputting',\n",
              " 'words',\n",
              " 'and',\n",
              " 'sentences',\n",
              " 'through',\n",
              " 'a',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " 'algorithm/model',\n",
              " 'to',\n",
              " 'generate',\n",
              " 'an',\n",
              " 'output',\n",
              " 'result',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "nltk.download('universal_tagset')\n",
        "from yellowbrick.text import PosTagVisualizer\n",
        "\n",
        "tokenizing = word_tokenize(sampletext)\n",
        "taging = pos_tag(tokenizing, tagset = \"universal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK-DEMRO1cG9",
        "outputId": "f5a3bcb7-955c-4980-c1b0-b5fad6d17629"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "print(punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLU7gFvFNlUP",
        "outputId": "394587c1-5628-42cf-f9b8-ce743c79b023"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordcount = {}\n",
        "for word in tokenizing:\n",
        "  if word not in wrds:\n",
        "    if word not in wordcount.keys():\n",
        "      wordcount[word] = 1\n",
        "    else:\n",
        "      wordcount[word] += 1\n",
        "print(wordcount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsGIXlC6OnP9",
        "outputId": "fb66b611-7fc3-4792-a84e-14e065595daa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This': 2, 'diagram': 1, 'known': 1, 'parse': 4, 'tree': 3, 'visual': 1, 'representation': 1, 'relationships': 2, 'words': 3, '.': 8, 'In': 2, 'particular': 1, 'example': 1, ',': 5, 'London': 2, 'relation': 1, 'capital': 2, 'From': 1, 'knowledge': 1, 'interpret': 1, 'intuitive': 1, 'way': 1, 'interpreting': 1, 'sentence': 1, 'However': 1, 'computer': 2, 'wouldn': 1, '’': 1, 't': 1, 'know': 2, 'word': 2, 'generate': 3, 'order': 1, 'maximum': 1, 'accuracy': 1, 'needs': 1, 'trained': 1, 'identify': 1, 'Similar': 1, 'part-of-speech': 1, 'portion': 1, 'NLP': 1, 'pipeline': 1, 'employed': 1, 'Machine': 2, 'Learning': 2, 'algorithms': 1, 'Dependency': 1, 'parsing': 1, 'works': 1, 'inputting': 1, 'sentences': 1, 'algorithm/model': 1, 'output': 1, 'result': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highestoccurence = max(wordcount.values())\n",
        "for word in wordcount:\n",
        "  wordcount[word] = wordcount[word]/highestoccurence\n",
        "print(wordcount)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BcwLOx2vj44",
        "outputId": "35101df0-da5b-4858-d405-be6118d1cf73"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This': 0.25, 'diagram': 0.125, 'known': 0.125, 'parse': 0.5, 'tree': 0.375, 'visual': 0.125, 'representation': 0.125, 'relationships': 0.25, 'words': 0.375, '.': 1.0, 'In': 0.25, 'particular': 0.125, 'example': 0.125, ',': 0.625, 'London': 0.25, 'relation': 0.125, 'capital': 0.25, 'From': 0.125, 'knowledge': 0.125, 'interpret': 0.125, 'intuitive': 0.125, 'way': 0.125, 'interpreting': 0.125, 'sentence': 0.125, 'However': 0.125, 'computer': 0.25, 'wouldn': 0.125, '’': 0.125, 't': 0.125, 'know': 0.25, 'word': 0.25, 'generate': 0.375, 'order': 0.125, 'maximum': 0.125, 'accuracy': 0.125, 'needs': 0.125, 'trained': 0.125, 'identify': 0.125, 'Similar': 0.125, 'part-of-speech': 0.125, 'portion': 0.125, 'NLP': 0.125, 'pipeline': 0.125, 'employed': 0.125, 'Machine': 0.25, 'Learning': 0.25, 'algorithms': 0.125, 'Dependency': 0.125, 'parsing': 0.125, 'works': 0.125, 'inputting': 0.125, 'sentences': 0.125, 'algorithm/model': 0.125, 'output': 0.125, 'result': 0.125}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(sampletext)\n",
        "\n",
        "\n",
        "# Calculate the score for each sentence\n",
        "sentence_scores = {}\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    sentence_score = 0\n",
        "    for token in tokens:\n",
        "        if token in wordcount:\n",
        "            sentence_score += wordcount[token]\n",
        "    # Add the score for the sentence to the dictionary\n",
        "    sentence_scores[sentence] = sentence_score\n",
        "print(sentence_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx4ybcUbx_Es",
        "outputId": "fb71855c-da05-4570-e6bf-02528bc1f6e7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This diagram above is known as a parse tree and it is a visual representation of the relationships between all words.': 3.25, 'In this particular example, we can see that London has a relation with be and capital.': 2.75, 'From this knowledge alone, we can interpret that London is a capital.': 2.5, 'This intuitive way of interpreting a parse tree can be used for any of the other words in this sentence.': 3.0, 'However, a computer wouldn’t just know how to parse each word nor will they know how to generate a parse tree.': 4.875, 'In order to do this to maximum accuracy, the computer needs to be trained to identify each word and generate these relationships.': 3.75, 'Similar to how the part-of-speech portion of the NLP pipeline employed Machine Learning algorithms, this part also does.': 3.0, 'Dependency parsing works by inputting words and sentences through a Machine Learning algorithm/model to generate an output result.': 3.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "rd_MjEVezNcM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "sentencecount = int(len(sentences))*0.25\n",
        "sentencecount = math.ceil(sentencecount)\n",
        "print(sentencecount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjjJPU9ozjSB",
        "outputId": "5c78556f-2b9f-410e-804f-415b47763686"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_summary = nlargest(sentencecount, sentence_scores, key = sentence_scores.get)\n",
        "print(list_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZG7wueA0Cna",
        "outputId": "27c1cfa6-b217-4d96-bacd-def3b80d56d0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['However, a computer wouldn’t just know how to parse each word nor will they know how to generate a parse tree.', 'In order to do this to maximum accuracy, the computer needs to be trained to identify each word and generate these relationships.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \" \".join(list_summary)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhUk_Xtl1KMP",
        "outputId": "a1d81c09-8f99-4539-b4de-4c41b601a2ef"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "However, a computer wouldn’t just know how to parse each word nor will they know how to generate a parse tree. In order to do this to maximum accuracy, the computer needs to be trained to identify each word and generate these relationships.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sampletext))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MUfSxHY14tm",
        "outputId": "f299b289-9aa9-4604-c33d-e00e3456909f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2iFO5rp17XR",
        "outputId": "e0e750c5-fd63-4aac-a2a5-424678a8d240"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240\n"
          ]
        }
      ]
    }
  ]
}